{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOPLku+60IocRoxbqA15G09",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnovaYoung/Natural-Language-Processing/blob/main/LLM_for_Text_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing and Evaluating a Large Language Model (LLM) for Text Classification"
      ],
      "metadata": {
        "id": "HGVTbejhuaSF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This assignment aims to provide hands-on experience with LLMs by implementing and evaluating a model for a text classification task."
      ],
      "metadata": {
        "id": "MdBWj-XtuGSc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "lZZ771i8okYd"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = '/content/archive (3).zip'\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "\n",
        "    zip_ref.extractall('/content/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data_path = '/content/complaints_processed.csv'\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQ2smbZZuc2X",
        "outputId": "8a677558-0696-401b-eb7f-6b50aeee79d1"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0           product  \\\n",
            "0           0       credit_card   \n",
            "1           1       credit_card   \n",
            "2           2    retail_banking   \n",
            "3           3  credit_reporting   \n",
            "4           4  credit_reporting   \n",
            "\n",
            "                                           narrative  \n",
            "0  purchase order day shipping amount receive pro...  \n",
            "1  forwarded message date tue subject please inve...  \n",
            "2  forwarded message cc sent friday pdt subject f...  \n",
            "3  payment history missing credit report speciali...  \n",
            "4  payment history missing credit report made mis...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the output, I can see that the DataFrame has three columns: Unnamed: 0, product, and narrative.\n",
        "\n"
      ],
      "metadata": {
        "id": "BgWa7Ot8yVw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "missing_values = df.isnull().sum()\n",
        "print(missing_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrRFyz0AyYqY",
        "outputId": "4c7ccbf2-883f-445c-cd61-cbcbce66c5ee"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unnamed: 0     0\n",
            "product        0\n",
            "narrative     10\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the output, I have 10 missing values in the narrative column. Since the narrative column contains the text data we'll use for classification, it's really important to handle these missing entries before proceeding.\n",
        "\n",
        "Before I remove any data, it's important to understand how many rows there are in total to ensure that dropping rows with missing values won't significantly impact the dataset."
      ],
      "metadata": {
        "id": "4YnI92nFzoDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the total number of rows before removing NaN values\n",
        "total_rows = df.shape[0]\n",
        "print(f\"Total number of rows before removing NaN values: {total_rows}\")\n",
        "\n",
        "# Number of missing 'narrative' entries\n",
        "missing_narratives = df['narrative'].isnull().sum()\n",
        "print(f\"Number of missing 'narrative' entries: {missing_narratives}\")\n",
        "\n",
        "# Calculate the percentage of missing narratives\n",
        "percentage_missing = (missing_narratives / total_rows) * 100\n",
        "print(f\"Percentage of missing 'narrative' entries: {percentage_missing:.4f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d570ZV9zrx0",
        "outputId": "0266b625-d4d9-4384-e59f-a5b69aeaa87d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of rows before removing NaN values: 162421\n",
            "Number of missing 'narrative' entries: 10\n",
            "Percentage of missing 'narrative' entries: 0.0062%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is great, removing the 10 rows will not impact the df at all, lets proceed."
      ],
      "metadata": {
        "id": "5KYWvtXJ0IEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove rows with missing 'narrative' values\n",
        "df = df.dropna(subset=['narrative'])\n",
        "\n",
        "# Verify that there are no more missing values in 'narrative'\n",
        "missing_values_after = df['narrative'].isnull().sum()\n",
        "print(f\"Missing values in 'narrative' after cleaning: {missing_values_after}\")\n",
        "\n",
        "# Updated total number of rows\n",
        "total_rows_after = df.shape[0]\n",
        "print(f\"Total number of rows after removing NaN values: {total_rows_after}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zCpq0oR0NKc",
        "outputId": "c8002e6e-19cf-4233-e543-f53d13f99879"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values in 'narrative' after cleaning: 0\n",
            "Total number of rows after removing NaN values: 162411\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, now I'm going to check for duplicates based on 'narrative' column"
      ],
      "metadata": {
        "id": "rOVUMlCp0fTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "duplicate_count = df.duplicated(subset=['narrative']).sum()\n",
        "print(f\"Number of duplicate narratives: {duplicate_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVpG8ViS0anQ",
        "outputId": "dce7ac79-7c52-4731-bcca-63f838b926af"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of duplicate narratives: 37939\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Percentage of duplicate narratives\n",
        "percentage_duplicates = (duplicate_count / df.shape[0]) * 100\n",
        "print(f\"Percentage of duplicate narratives: {percentage_duplicates:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Frp_wuzO0_KH",
        "outputId": "8d073f67-aec1-45b0-b42f-151bbe0f6ca9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of duplicate narratives: 23.36%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Total Rows: After removing missing values, we have 162,421 rows.\n",
        "\n",
        "Duplicate Narratives: 37,939\n",
        "\n",
        "Impact: 23.36% of the dataset consists of duplicate narratives.\n",
        "\n",
        "Thats definetly not an insignificant number.\n",
        "\n",
        "There are many different ways to deal with this, including investigating product labels and looking at class distribution. I'll look at class distribution in a moment but for this purposes of this project I am simply going to remove duplicate narratives, so there is only one unique occurance."
      ],
      "metadata": {
        "id": "3ZQu_5Kp0xbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function removes duplicate rows based on the 'narrative' column.\n",
        "# The parameter keep='first' ensures that the first occurrence of each narrative is kept, and subsequent duplicates are dropped.\n",
        "df = df.drop_duplicates(subset=['narrative'], keep='first')\n",
        "\n",
        "# Verify that there are no more duplicate narratives\n",
        "duplicate_narratives = df.duplicated(subset=['narrative']).sum()\n",
        "print(f\"Number of duplicate narratives after removing duplicates: {duplicate_narratives}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwt_y2Ue3MPH",
        "outputId": "e48c8d15-f063-4c3f-8b9e-34792adcdc49"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of duplicate narratives after removing duplicates: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "df.drop_duplicates(subset=['narrative'], keep='first'):\n",
        "\n",
        "This function removes duplicate rows based on the 'narrative' column.\n",
        "\n",
        "The parameter keep='first' ensures that the first occurrence of each narrative is kept, and subsequent duplicates are dropped."
      ],
      "metadata": {
        "id": "EplNMe4N3e83"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that I've removed duplicate narratives, I will examine the distribution of the classes in the 'product' column to understand how the data is spread across different categories."
      ],
      "metadata": {
        "id": "wJhO_EfnfEEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the count of each class in 'product'\n",
        "class_distribution = df['product'].value_counts()\n",
        "\n",
        "# Print the class distribution\n",
        "print(\"Class Distribution:\")\n",
        "print(class_distribution)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNA5rWUQ3a-6",
        "outputId": "c2d976ac-e163-4505-9a48-6cd7d8aa69c2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Distribution:\n",
            "product\n",
            "credit_reporting       56240\n",
            "debt_collection        21057\n",
            "mortgages_and_loans    18723\n",
            "credit_card            14983\n",
            "retail_banking         13469\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "'credit_reporting' has the highest number of complaints at 56,240 entries.\n",
        "\n",
        "'retail_banking' has the lowest number of complaints at 13,469 entries.\n",
        "\n",
        "There is a noticeable imbalance among the classes.\n",
        "\n",
        "Let's get the percentage representation of each class to quantify the imbalance"
      ],
      "metadata": {
        "id": "pNPxSKlCf-RK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate total number of entries\n",
        "total_entries = df.shape[0]\n",
        "\n",
        "# Calculate percentage for each class\n",
        "class_percentage = (class_distribution / total_entries) * 100\n",
        "\n",
        "# Print class percentages\n",
        "print(\"Class Percentages:\")\n",
        "print(class_percentage)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDY4kRE4gIB3",
        "outputId": "9cfdeb14-e174-40a3-c6b8-66805830764a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Percentages:\n",
            "product\n",
            "credit_reporting       45.182852\n",
            "debt_collection        16.917058\n",
            "mortgages_and_loans    15.041937\n",
            "credit_card            12.037245\n",
            "retail_banking         10.820908\n",
            "Name: count, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will deal with this by adjusting class weights to facor the minority. I will do this later.\n",
        "\n",
        "For now let's do some preprocessing:\n",
        "\n",
        "Converting text to lowercase: Since I'm using an uncased model (bert-base-uncased), I'll convert all text to lowercase.\n",
        "\n",
        "Removing leading and trailing whitespace: Ensures consistency in text formatting.\n",
        "\n",
        "Replacing multiple spaces with a single space: Cleans up any irregular spacing."
      ],
      "metadata": {
        "id": "nJKM5zagk35C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert narratives to lowercase\n",
        "df['narrative'] = df['narrative'].str.lower()\n",
        "\n",
        "# Remove leading and trailing whitespace\n",
        "df['narrative'] = df['narrative'].str.strip()\n",
        "\n",
        "# Replace multiple spaces with a single space\n",
        "df['narrative'] = df['narrative'].str.replace('\\s+', ' ', regex=True)\n"
      ],
      "metadata": {
        "id": "ZQyd8R3rlzzf"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sample narratives after preprocessing:\")\n",
        "print(df['narrative'].head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCpZ3hmBmOBz",
        "outputId": "db42a37b-775f-47db-fbc4-4c5d79f7d777"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample narratives after preprocessing:\n",
            "0    purchase order day shipping amount receive pro...\n",
            "1    forwarded message date tue subject please inve...\n",
            "2    forwarded message cc sent friday pdt subject f...\n",
            "3    payment history missing credit report speciali...\n",
            "4    payment history missing credit report made mis...\n",
            "Name: narrative, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label Encoding is the next step."
      ],
      "metadata": {
        "id": "m91m3GdtplO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Always initialize the label encoder first\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the 'product' column to encode labels\n",
        "df['label'] = label_encoder.fit_transform(df['product'])\n",
        "\n",
        "# Map encoded labels back to original labels for reference\n",
        "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "\n",
        "# Display the label mapping\n",
        "print(\"Label Mapping:\")\n",
        "for product, label in label_mapping.items():\n",
        "    print(f\"'{product}': {label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-q8FxpnFptIB",
        "outputId": "d688974c-b7f6-46dc-d288-b7bbdcab8386"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label Mapping:\n",
            "'credit_card': 0\n",
            "'credit_reporting': 1\n",
            "'debt_collection': 2\n",
            "'mortgages_and_loans': 3\n",
            "'retail_banking': 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the Data into Training and Testing Sets"
      ],
      "metadata": {
        "id": "RmpFbgnmq4Mw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df['narrative'].values\n",
        "y = df['label'].values\n",
        "\n",
        "# Split the data into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "# create DataFrames for the splits\n",
        "df_train = pd.DataFrame({'narrative': X_train, 'label': y_train})\n",
        "df_test = pd.DataFrame({'narrative': X_test, 'label': y_test})\n",
        "\n",
        "# Verify the size of the splits\n",
        "print(f\"Training set size: {df_train.shape[0]} records\")\n",
        "print(f\"Testing set size: {df_test.shape[0]} records\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyC8A9zDq2q-",
        "outputId": "a2e7f62c-c66d-47ec-c2a2-e6076671ddc8"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 99577 records\n",
            "Testing set size: 24895 records\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computing class weights helps address class imbalance by assigning higher weights to minority classes during model training. This ensures the model pays more attention to underrepresented classes."
      ],
      "metadata": {
        "id": "8w8FCprlrP8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "# Get the unique class labels from the training set\n",
        "class_labels = np.unique(y_train)\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=class_labels,\n",
        "    y=y_train\n",
        ")\n",
        "\n",
        "# Create a dictionary that maps the labels to the weights\n",
        "class_weights_dict = dict(zip(class_labels, class_weights))\n",
        "\n",
        "# Display the class weights with corresponding product names\n",
        "print(\"Class Weights:\")\n",
        "for label, weight in class_weights_dict.items():\n",
        "    product = list(label_mapping.keys())[list(label_mapping.values()).index(label)]\n",
        "    print(f\"Class '{product}' (Label {label}): Weight {weight:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWRJGhrxrQod",
        "outputId": "200984e0-cdbf-4385-c02d-dcfa30a58b68"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Weights:\n",
            "Class 'credit_card' (Label 0): Weight 1.66\n",
            "Class 'credit_reporting' (Label 1): Weight 0.44\n",
            "Class 'debt_collection' (Label 2): Weight 1.18\n",
            "Class 'mortgages_and_loans' (Label 3): Weight 1.33\n",
            "Class 'retail_banking' (Label 4): Weight 1.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now ill prepare the data for training by tokenizing the text using BERT's tokenizer"
      ],
      "metadata": {
        "id": "wycv-zBKr_Aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizerFast\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the training data\n",
        "train_encodings = tokenizer(\n",
        "    list(X_train),\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=128\n",
        ")\n",
        "\n",
        "# Tokenize the testing data\n",
        "test_encodings = tokenizer(\n",
        "    list(X_test),\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=128\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSO7fLRYr_kk",
        "outputId": "2c45b67e-4a36-482a-b761-373775834754"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's proceed to convert the tokenized data into PyTorch tensors and prepare the datasets for model training."
      ],
      "metadata": {
        "id": "dDX3eo6ts01z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Convert tokenized inputs to tensors\n",
        "train_input_ids = torch.tensor(train_encodings['input_ids'])\n",
        "train_attention_masks = torch.tensor(train_encodings['attention_mask'])\n",
        "train_labels = torch.tensor(y_train)\n",
        "\n",
        "test_input_ids = torch.tensor(test_encodings['input_ids'])\n",
        "test_attention_masks = torch.tensor(test_encodings['attention_mask'])\n",
        "test_labels = torch.tensor(y_test)\n",
        "\n",
        "# Create TensorDatasets\n",
        "train_dataset = torch.utils.data.TensorDataset(\n",
        "    train_input_ids, train_attention_masks, train_labels\n",
        ")\n",
        "\n",
        "test_dataset = torch.utils.data.TensorDataset(\n",
        "    test_input_ids, test_attention_masks, test_labels\n",
        ")\n"
      ],
      "metadata": {
        "id": "VB-CWiL6s4GN"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating dsataloaders:\n",
        "\n",
        "DataLoaders:\n",
        "Facilitate batch processing and shuffling of data during training.\n",
        "\n",
        "Samplers:\n",
        "RandomSampler: Randomly samples elements for training data.\n",
        "\n",
        "SequentialSampler: Samples elements sequentially for testing data."
      ],
      "metadata": {
        "id": "kLYDf-a3ta04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 16\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    sampler=RandomSampler(train_dataset),\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    sampler=SequentialSampler(test_dataset),\n",
        "    batch_size=batch_size\n",
        ")\n"
      ],
      "metadata": {
        "id": "jBeTcfbXti6h"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up model"
      ],
      "metadata": {
        "id": "jt6I9FNmtmA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "# Number of labels\n",
        "num_labels = len(label_encoder.classes_)\n",
        "\n",
        "# Load pre-trained BERT model with the number of output labels\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels=num_labels\n",
        ")\n",
        "\n",
        "# Move model to device (CPU or GPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzowaknftniP",
        "outputId": "c638e461-5b4a-471c-9eda-70be9b7aba52"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the Optimizer and Scheduler"
      ],
      "metadata": {
        "id": "vFAa-Y6-uXaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# Total number of training steps\n",
        "epochs = 3\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Define scheduler\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81wlsyyxuX3h",
        "outputId": "498368fc-b513-4408-f0c6-fb740a5016ae"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modify the Loss Function to Include Class Weights"
      ],
      "metadata": {
        "id": "pdm1tsq0uc1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "# Convert class weights to tensor\n",
        "class_weights_tensor = torch.tensor(list(class_weights_dict.values()), dtype=torch.float).to(device)\n",
        "\n",
        "# Custom training loop\n",
        "def train_epoch(model, dataloader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(\n",
        "            b_input_ids,\n",
        "            attention_mask=b_input_mask,\n",
        "            labels=b_labels\n",
        "        )\n",
        "\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Compute loss with class weights\n",
        "        loss_fct = CrossEntropyLoss(weight=class_weights_tensor)\n",
        "        loss = loss_fct(logits.view(-1, num_labels), b_labels.view(-1))\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return avg_loss\n"
      ],
      "metadata": {
        "id": "Y9eMKQ5RudgI"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the Model\n",
        "\n",
        "I tried to run the model and it was too long. So im going to try training on a smaller dataset which will significantly reduce the training time. I'll take a random sample of your training data."
      ],
      "metadata": {
        "id": "B9D1Yz9fvF1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample 10% of the training data\n",
        "df_train_sample = df_train.sample(frac=0.1, random_state=42)\n",
        "\n",
        "# Update X_train and y_train\n",
        "X_train_sample = df_train_sample['narrative'].values\n",
        "y_train_sample = df_train_sample['label'].values\n",
        "\n",
        "# Tokenize the sampled training data\n",
        "train_encodings_sample = tokenizer(\n",
        "    list(X_train_sample),\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=128\n",
        ")\n",
        "\n",
        "# Convert to tensors\n",
        "train_input_ids_sample = torch.tensor(train_encodings_sample['input_ids'])\n",
        "train_attention_masks_sample = torch.tensor(train_encodings_sample['attention_mask'])\n",
        "train_labels_sample = torch.tensor(y_train_sample)\n",
        "\n",
        "# Create TensorDataset\n",
        "train_dataset_sample = torch.utils.data.TensorDataset(\n",
        "    train_input_ids_sample, train_attention_masks_sample, train_labels_sample\n",
        ")\n",
        "\n",
        "# Update DataLoader with the sampled dataset\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset_sample,\n",
        "    sampler=RandomSampler(train_dataset_sample),\n",
        "    batch_size=batch_size\n",
        ")\n"
      ],
      "metadata": {
        "id": "MZ_h66Vxx10k"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "frac=0.1: Samples 10% of the data.\n",
        "Adjust frac as needed: You can increase or decrease this fraction based on how much data you want to use."
      ],
      "metadata": {
        "id": "3tueFoYWyZKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 2  # Reduce the number of epochs to run quicker\n"
      ],
      "metadata": {
        "id": "kDqqYgjSyTyU"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reduce the maximum sequence length in tokenization.\n"
      ],
      "metadata": {
        "id": "07NqYZXwygJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a smaller max_length\n",
        "train_encodings = tokenizer(\n",
        "    list(X_train),\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=64\n",
        ")\n",
        "\n",
        "test_encodings = tokenizer(\n",
        "    list(X_test),\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=64\n",
        ")\n",
        ""
      ],
      "metadata": {
        "id": "FrqX4jYIyg97"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a smaller model like DistilBERT can significantly speed up training."
      ],
      "metadata": {
        "id": "QOpuStjPy0F1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast\n",
        "\n",
        "# Load the DistilBERT tokenizer and model\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\n",
        "    'distilbert-base-uncased',\n",
        "    num_labels=num_labels\n",
        ")\n",
        "\n",
        "# Move model to device\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l09GKPLRy0sL",
        "outputId": "586e09b7-8493-4180-d147-a667a65a5ab0"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertForSequenceClassification(\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Accumalation: Simulate a larger batch size with smaller batches."
      ],
      "metadata": {
        "id": "JSIG0PtDy-p-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_accumulation_steps = 2\n",
        "\n",
        "def train_epoch(model, dataloader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for step, batch in enumerate(dataloader):\n",
        "        b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "        outputs = model(\n",
        "            b_input_ids,\n",
        "            attention_mask=b_input_mask,\n",
        "            labels=b_labels\n",
        "        )\n",
        "        loss = outputs.loss / gradient_accumulation_steps\n",
        "        loss.backward()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if (step + 1) % gradient_accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return avg_loss\n"
      ],
      "metadata": {
        "id": "vhK2VVH3y_VG"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CP5qIBeyzL9w",
        "outputId": "9947d397-03b4-4f96-9429-37e06d2d167c"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use a fixed number of samples for quick experimentation."
      ],
      "metadata": {
        "id": "NWE-GgKkzS-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use only the first 5000 samples for training\n",
        "X_train_small = X_train[:5000]\n",
        "y_train_small = y_train[:5000]\n",
        "\n",
        "# Tokenize the smaller training set\n",
        "train_encodings_small = tokenizer(\n",
        "    list(X_train_small),\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=128\n",
        ")\n",
        "\n",
        "# Convert to tensors and create dataset\n",
        "train_input_ids_small = torch.tensor(train_encodings_small['input_ids'])\n",
        "train_attention_masks_small = torch.tensor(train_encodings_small['attention_mask'])\n",
        "train_labels_small = torch.tensor(y_train_small)\n",
        "\n",
        "train_dataset_small = torch.utils.data.TensorDataset(\n",
        "    train_input_ids_small, train_attention_masks_small, train_labels_small\n",
        ")\n",
        "\n",
        "# Update DataLoader\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset_small,\n",
        "    sampler=RandomSampler(train_dataset_small),\n",
        "    batch_size=batch_size\n",
        ")\n"
      ],
      "metadata": {
        "id": "5QR4CH5yzP12"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize sampled training data\n",
        "train_encodings_sample = tokenizer(\n",
        "    list(X_train_sample),\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=128\n",
        ")\n",
        "\n",
        "# Tokenize the test data (using the full test set)\n",
        "test_encodings = tokenizer(\n",
        "    list(X_test),\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=128\n",
        ")\n"
      ],
      "metadata": {
        "id": "PlzCxXEU2AhG"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training tensors\n",
        "train_input_ids = torch.tensor(train_encodings_sample['input_ids'])\n",
        "train_attention_masks = torch.tensor(train_encodings_sample['attention_mask'])\n",
        "train_labels = torch.tensor(y_train_sample)\n",
        "\n",
        "# Testing tensors\n",
        "test_input_ids = torch.tensor(test_encodings['input_ids'])\n",
        "test_attention_masks = torch.tensor(test_encodings['attention_mask'])\n",
        "test_labels = torch.tensor(y_test)\n"
      ],
      "metadata": {
        "id": "k2Y8zQRx2GnL"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training dataset\n",
        "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
        "\n",
        "#testing dataset\n",
        "test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n"
      ],
      "metadata": {
        "id": "crj8irM02JpB"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoaders\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    sampler=RandomSampler(train_dataset),\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    sampler=SequentialSampler(test_dataset),\n",
        "    batch_size=batch_size\n",
        ")\n"
      ],
      "metadata": {
        "id": "jtY7xi_D2OgC"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set Up the Optimizer and Scheduler"
      ],
      "metadata": {
        "id": "Jzw9XkMq2cCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# Set the number of epochs\n",
        "epochs = 2\n",
        "\n",
        "# Total number of training steps\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# this is the learning rate schedule\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")\n"
      ],
      "metadata": {
        "id": "FtUbPscA2Z6v"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Re-Modify the Loss Function to Include Class Weights"
      ],
      "metadata": {
        "id": "36GJ6Cmy2fRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "# Ensure class_weights_dict uses labels from 0 to num_labels - 1\n",
        "class_weights_list = [class_weights_dict[i] for i in range(num_labels)]\n",
        "\n",
        "# Convert class weights to a tensor\n",
        "class_weights_tensor = torch.tensor(class_weights_list, dtype=torch.float).to(device)\n"
      ],
      "metadata": {
        "id": "z5qEGQ7b2gxa"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the Training Loop"
      ],
      "metadata": {
        "id": "-poDFB4h2is7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, dataloader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(\n",
        "            b_input_ids,\n",
        "            attention_mask=b_input_mask,\n",
        "            labels=b_labels\n",
        "        )\n",
        "\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Compute loss with class weights\n",
        "        loss_fct = CrossEntropyLoss(weight=class_weights_tensor)\n",
        "        loss = loss_fct(logits.view(-1, num_labels), b_labels.view(-1))\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return avg_loss\n"
      ],
      "metadata": {
        "id": "DOFkN9qZ2kbI"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    predictions, true_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            outputs = model(\n",
        "                b_input_ids,\n",
        "                attention_mask=b_input_mask\n",
        "            )\n",
        "\n",
        "            logits = outputs.logits\n",
        "            predictions.append(logits.detach().cpu())\n",
        "            true_labels.append(b_labels.cpu())\n",
        "\n",
        "    predictions = torch.cat(predictions, dim=0)\n",
        "    true_labels = torch.cat(true_labels, dim=0)\n",
        "\n",
        "    preds_flat = torch.argmax(predictions, axis=1).flatten()\n",
        "    labels_flat = true_labels.flatten()\n",
        "\n",
        "    accuracy = accuracy_score(labels_flat, preds_flat)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        labels_flat, preds_flat, average='weighted'\n",
        "    )\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "JGFI9q4K2oMN"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "    avg_train_loss = train_epoch(model, train_dataloader)\n",
        "    print(f\"Average training loss: {avg_train_loss:.4f}\\n\")\n",
        "\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Evaluating the model on the test set:\")\n",
        "evaluate(model, test_dataloader)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHrmWEP32zzo",
        "outputId": "83333b78-1c43-4388-80dc-1fdbf92bd6ac"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "Average training loss: 0.8278\n",
            "\n",
            "Epoch 2/2\n",
            "Average training loss: 0.4986\n",
            "\n",
            "Training complete!\n",
            "Evaluating the model on the test set:\n",
            "Accuracy: 0.8004\n",
            "Precision: 0.8141\n",
            "Recall: 0.8004\n",
            "F1-Score: 0.8027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analysis of the Model Output:**\n",
        "\n",
        "Training Loss:\n",
        "\n",
        "Epoch 1 Average Loss: 0.8278\n",
        "\n",
        "Epoch 2 Average Loss: 0.4986\n",
        "\n",
        "Observation: The training loss decreased significantly from the first to the second epoch, the model is learning effectively from the data.\n",
        "Evaluation Metrics on Test Set:\n",
        "\n",
        "Accuracy: 80.04%\n",
        "\n",
        "Precision: 81.41%\n",
        "\n",
        "Recall: 80.04%\n",
        "\n",
        "F1-Score: 80.27%\n",
        "\n",
        "Observation: The model achieves solid performance across all metrics, especially considering the reduced dataset size and limited training epochs.\n",
        "Conclusions:\n",
        "\n",
        "The decrease in training loss shows effective learning, and the reasonable loss values suggest the model isn't overfitting.\n",
        "\n",
        "An accuracy of ~80% is respectable for a text classification task with multiple classes.\n",
        "\n",
        "The close values of precision, recall, and F1-score indicate balanced performance without significant bias toward any class.\n",
        "\n",
        "Next Steps:\n",
        "\n",
        "Increase Training Data: Using more training samples could further improve performance.\n",
        "\n",
        "Adjust Epochs: Training for additional epochs would definetly enhance learning.\n",
        "\n",
        "Hyperparameter Tuning: Experimenting with learning rates, batch sizes, or using a different model can yield better results. Especially since i immedietly had to redo the and retune the model siince it took an hour originally to go nowhere.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eR6UtSr54ts_"
      }
    }
  ]
}